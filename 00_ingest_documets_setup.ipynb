{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # !pip install -U langchain pypdfium2\n",
    "# %pip install streamlit --upgrade\n",
    "# %pip install streamlit-authenticator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import streamlit_authenticator as stauth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamlit_authenticator.utilities.hasher import Hasher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import streamlit_authenticator as stauth\n",
    "# hashed_passwords = stauth.Hasher(['streamlit']).generate()\n",
    "# hashed_passwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$2b$12$q.eUqMDZLuMjs1EdmT50O.K01JfrQQRTDY9C37Tf8uR6sqK1Xom6m', '$2b$12$x3h0ixsvoh5K1cv1aXAa3enfwlOVAtZyttSt3BrZqxg0O3gfqjyYK', '$2b$12$TunvzL7FNTEuMarCc4oQ.uOZBnRF8u2mAOoDbdDYlh1sQ9IClsxM6']\n"
     ]
    }
   ],
   "source": [
    "from streamlit_authenticator.utilities.hasher import Hasher\n",
    "\n",
    "# Create an instance of the Hasher class with a list of passwords\n",
    "passwords = ['password1', 'password250', ''] ## add your pwd examples\n",
    "hasher = Hasher(passwords)\n",
    "\n",
    "# Call the generate method on the Hasher instance\n",
    "hashed_passwords = hasher.generate()\n",
    "\n",
    "print(hashed_passwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader, PyPDFLoader, PyPDFium2Loader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import boto3, json, ast, logging, time, re\n",
    "\n",
    "import re\n",
    "import time\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]C:\\ProgramData\\anaconda3\\envs\\lang-pip\\Lib\\site-packages\\pypdfium2\\_helpers\\textpage.py:81: UserWarning: get_text_range() call with default params will be implicitly redirected to get_text_bounded()\n",
      "  warnings.warn(\"get_text_range() call with default params will be implicitly redirected to get_text_bounded()\")\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 10.11it/s]\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "for file_name in tqdm(glob('./Data/*')):\n",
    "    loader = PyPDFium2Loader(file_name)\n",
    "    doc_pages = loader.load()\n",
    "    docs += doc_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_docs = [doc for doc in docs if len(doc.page_content)>10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the boto3 session and lambda client\n",
    "session = boto3.Session(profile_name=, region_name='us-east-1') ## add your profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"amazon.titan-embed-text-v1\"\n",
    "be = BedrockEmbeddings(\n",
    "    model_id=model_id,\n",
    "    credentials_profile_name=, region_name=\"us-east-1\"\n",
    "    \n",
    ") ## add your profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index = FAISS.from_documents(x_docs, be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index.save_local(\"vectorstore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import BedrockChat\n",
    "from typing import Any, Dict, Optional\n",
    "from botocore.client import Config\n",
    "\n",
    "bedrock_config = Config(connect_timeout=180, read_timeout=180, retries={'max_attempts': 5})\n",
    "bedrock_client = session.client('bedrock-runtime')\n",
    "\n",
    "bedrock_model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"  # Bedrock model_id\n",
    "\n",
    "model_kwargs_claude = {\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_p\": 0.99,\n",
    "    \"max_tokens\": 3000,\n",
    "    \"stop_sequences\": [\"\\n\\nHuman\"]\n",
    "}\n",
    "\n",
    "\n",
    "llm =  BedrockChat(model_id=bedrock_model_id, \n",
    "                   client=bedrock_client,\n",
    "                   model_kwargs=model_kwargs_claude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are a helpful assistant that answers questions directly and only using the information provided in the context below. \n",
    "    Guidance for answers:\n",
    "        - Always use English as the language in your responses.\n",
    "        - In your answers, always use a professional tone.\n",
    "        - Begin your answers with \"Based on the context provided: \"\n",
    "        - Simply answer the question clearly and with lots of detail using only the relevant details from the information below. If the context does not contain the answer, say \"Sorry, I didn't understand that. Could you rephrase your question?\"\n",
    "        - Use bullet-points and provide as much detail as possible in your answer. \n",
    "        - Always provide a summary at the end of your answer.\n",
    "        \n",
    "    Now read this context below and answer the question at the bottom.\n",
    "    \n",
    "    Context: {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \n",
    "    think step by step and answer the question with utmost precision and accuracy.\n",
    "    \n",
    "    Assistant:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\nYou are a knowledgeable AI assistant named AWS Support AI, specializing in AWS services and solutions. Your primary role is to assist AWS employees by providing accurate and concise guidance on AWS offerings, including but not limited to Amazon Aurora, Amazon Bedrock, and other AWS managed services\\n\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \"\"\" \n",
    " \n",
    " You are a knowledgeable AI assistant named AWS Support AI, specializing in AWS services and solutions. Your primary role is to assist AWS employees by providing accurate and concise guidance on AWS offerings, including but not limited to Amazon Aurora, Amazon Bedrock, and other AWS managed services\n",
    " \n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=faiss_index.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 5}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": PROMPT,\n",
    "        # \"memory\": ConversationBufferMemory(memory_key=\"history\", input_key=\"question\"),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is Aurora Standard and Aurora I/O-Optimized?',\n",
       " 'result': 'Based on the context provided:\\n\\nAurora offers two configuration options for optimizing your database spend:\\n\\nAurora Standard:\\n- A cost-effective configuration for applications with low to moderate I/O usage\\n- You pay for database instances, storage, and pay-per-request I/O operations\\n\\nAurora I/O-Optimized:\\n- Delivers improved price performance for I/O-intensive applications\\n- Ideal for workloads with high I/O variability or where I/O spend exceeds 25% of total Aurora costs\\n- Offers predictable pricing with no charges for read and write I/O operations\\n\\nKey differences:\\n- Aurora Standard charges for I/O operations, while Aurora I/O-Optimized does not\\n- Aurora I/O-Optimized is better suited for I/O-intensive workloads to provide cost savings\\n\\nSummary:\\n- Aurora Standard is a cost-effective configuration for low to moderate I/O usage\\n- Aurora I/O-Optimized is optimized for I/O-intensive workloads and offers predictable pricing with no I/O charges\\n- Customers can choose the configuration that best fits their price-performance and price-predictability needs.',\n",
       " 'source_documents': [Document(page_content='Concurrent write operations whose log records are less than 4 KB might be batched together by \\r\\nthe Aurora database engine in order to optimize I/O consumption. Unlike traditional database \\r\\nengines, Aurora never flushes dirty data pages to storage.\\r\\nYou can see how many I/O requests your Aurora instance is consuming by checking the AWS \\r\\nManagement Console. To find your I/O consumption, go to the Amazon RDS section of the \\r\\nconsole, look at your list of instances, select your Aurora instances, then look for the \\r\\n“VolumeReadIOPs” and “VolumeWriteIOPs” metrics in the monitoring section.\\r\\nFor more information on the pricing of I/O operations, visit the Aurora pricing page. You are \\r\\ncharged for read and write I/O operations when you configure your database clusters to the \\r\\nAurora Standard configuration. You are not charged for read and write I/O operations when you \\r\\nconfigure your database clusters to Amazon Aurora I/O-Optimized.\\r\\nWhat is Aurora Standard and Aurora I/O-Optimized?\\r\\nAurora offers you the flexibility to optimize your database spend by choosing between two \\r\\nconfiguration options based on your price-performance and price-predictability needs. The two \\r\\nconfiguration options are Aurora Standard and Aurora I/O-Optimized. Neither option requires \\r\\nupfront I/O or storage provisioning and both can scale I/O operations to support your most \\r\\ndemanding applications.\\r\\nAurora Standard is a database cluster configuration that offers cost-effective pricing for the vast \\r\\nmajority of applications with low to moderate I/O usage. With Aurora Standard, you pay for \\r\\ndatabase instances, storage, and pay-per-request I/O.\\r\\nAurora I/O-Optimized is a database cluster configuration that delivers improved price \\r\\nperformance for I/O-intensive applications such as payment processing systems, ecommerce \\r\\nsystems, and financial applications. Also, if your I/O spend exceeds 25% of your total Aurora \\r\\ndatabase spend, you can save up to 40% on costs for I/O-intensive workloads with Aurora I/O\\x02Optimized. Aurora I/O-Optimized offers predictable pricing for all applications as there are no \\r\\ncharges for read and write I/O operations, making this configuration ideal for workloads with \\r\\nhigh I/O variability.\\r\\nWhen should I use Aurora I/O-Optimized? \\r\\nAurora I/O-Optimized is the ideal choice when you need predictable costs for any application. It \\r\\ndelivers improved price performance for I/O-intensive applications, which require a high write \\r\\nthroughput or run analytical queries processing large amounts of data. For customers with an I/O \\r\\nspend that exceeds 25% of their Aurora bill, you can save up to 40% on costs for I/O-intensive \\r\\nworkloads with Aurora I/O-Optimized.\\r\\nHow do I migrate my existing database cluster to use Aurora I/O-Optimized?\\n', metadata={'source': './Data\\\\Amazon Aurora FAQs.pdf', 'page': 4}),\n",
       "  Document(page_content='You can use the one-click experience available in the AWS Management Console to change the \\r\\nstorage type of your existing database clusters to be Aurora I/O-Optimized. You can also invoke \\r\\nthe AWS Command Line Interface (AWS CLI) or AWS SDK to make this change. \\r\\nCan I switch back and forth between Aurora I/O-Optimized and Aurora \\r\\nStandard configuration?\\r\\nYou can switch your existing database clusters once every 30 days to Aurora I/O-Optimized. \\r\\nYou can switch back to Aurora Standard at any time. \\r\\nDoes Aurora I/O-Optimized work with Reserved Instances?\\r\\nYes, Aurora I/O-Optimized works with existing Aurora Reserved Instances. Aurora \\r\\nautomatically accounts for the price difference between Aurora Standard and Aurora I/O\\x02Optimized with Reserved Instances. With Reserved Instance discounts with Aurora I/O\\x02Optimized, you can gain even more savings on your I/O spend.\\r\\nDoes the price of backtrack, snapshot, export, or continuous backup change with \\r\\nAurora I/O-Optimized?\\r\\nThere are no changes to the price of backtrack, snapshot, export, or continuous backup with \\r\\nAurora I/O-Optimized. \\r\\nDo I continue paying for the I/O operations required for replicating data across \\r\\nRegions with Aurora Global Database with Aurora I/O-Optimized?\\r\\nYes, the charges for the I/O operations required to replicate data across Regions continue to \\r\\napply. Aurora I/O-Optimized does not charge for read and write I/O operations, which is \\r\\ndifferent from data replication. \\r\\nHardware and scaling\\r\\nWhat are the minimum and maximum storage limits of an Amazon Aurora \\r\\ndatabase?\\r\\nThe minimum storage is 10 GB. Based on your database usage, your Amazon Aurora storage \\r\\nwill automatically grow, up to 128 TiB, in 10 GB increments with no impact to database \\r\\nperformance. There is no need to provision storage in advance.\\r\\nHow do I scale the compute resources associated with my Amazon Aurora DB \\r\\nInstance?\\r\\nThere are two ways to scale the compute resources associated with my Amazon Aurora DB \\r\\nInstance – via Aurora Serverless and via manual adjustment.\\n', metadata={'source': './Data\\\\Amazon Aurora FAQs.pdf', 'page': 5}),\n",
       "  Document(page_content='Billing\\r\\nHow much does Aurora cost?\\r\\nSee the Aurora pricing page for current pricing information.\\r\\nDoes Aurora participate in the AWS Free Tier?\\r\\nThere is no AWS Free Tier offering for Aurora at this time. However, Aurora durably stores \\r\\nyour data across three Availability Zones in a Region and charges for only one copy of data. You \\r\\nare not charged for backups of up to 100% of the size of your database cluster. You are also not \\r\\ncharged for snapshots during the backup retention period that you’ve configured for your \\r\\ndatabase cluster.\\r\\nAurora replicates my data across three Availability Zones. Does that mean that \\r\\nmy effective storage price will be three times what is shown on the pricing page?\\r\\nNo, Aurora replication is bundled into the price. You are charged based on the storage your \\r\\ndatabase consumes at the database layer, not the storage consumed in the virtualized storage \\r\\nlayer of Aurora.\\r\\nWhat are I/O operations in Aurora and how are they calculated?\\r\\nI/O operations are performed by the Aurora database engine against its SSD-based virtualized \\r\\nstorage layer. Every database page read operation counts as one I/O.\\r\\nThe Aurora database engine issues reads against the storage layer to fetch database pages not \\r\\npresent in memory in the cache:\\r\\n• If your query traffic can be totally served from memory or the cache, you will not be \\r\\ncharged for retrieving any data pages from memory.\\r\\n• If your query traffic cannot be served en3rely from memory, you will be charged for any \\r\\ndata pages that need to be retrieved from storage.\\r\\nEach database page is 16 KB in Amazon Aurora MySQL-Compa3ble Edi3on and 8 KB in \\r\\nAurora PostgreSQL-Compa3ble Edi3on.\\r\\nAurora was designed to remove unnecessary I/O operations to reduce costs and ensure resources \\r\\nare available for serving read/write traffic. Write I/O operations are only consumed when \\r\\npersisting redo log records in Aurora MySQL-Compatible Edition or write ahead log records in \\r\\nAurora PostgreSQL-Compatible Edition to the storage layer for the purpose of making writes \\r\\ndurable.\\r\\nWrite I/O operations are counted in 4 KB units. For example, a log record that is 1,024 bytes \\r\\ncounts as one write I/O operation. However, if the log record is larger than 4 KB, more than one\\r\\nwrite I/O operation is needed to persist it.\\n', metadata={'source': './Data\\\\Amazon Aurora FAQs.pdf', 'page': 3}),\n",
       "  Document(page_content=\"In addition, it scales database capacity in increments as small as 0.5 Aurora Capacity Units \\r\\n(ACUs) so your database capacity closely matches your application’s needs.\\r\\nAurora Serverless v1 is a simple, cost-effective option for infrequent, intermittent, or \\r\\nunpredictable workloads. It automatically starts up, scales compute capacity to match your \\r\\napplication's usage, and shuts down when it's not in use. Visit the Aurora User Guide to learn \\r\\nmore.\\r\\nWhich Aurora features does Aurora Serverless v2 support?\\r\\nAurora Serverless v2 supports all features of provisioned Aurora, including read replica, Multi\\x02AZ configuration, Aurora Global Database, RDS Proxy, and Performance Insights.\\r\\nCan I start using Aurora Serverless v2 with provisioned instances in my existing \\r\\nAurora DB cluster?\\r\\nYes, you can start using Aurora Serverless v2 to manage database compute capacity in your \\r\\nexisting Aurora DB cluster. A cluster containing both provisioned instances as well as Aurora \\r\\nServerless v2 is referred to as a mixed-configuration cluster. You can choose to have any \\r\\ncombination of provisioned instances and Aurora Serverless v2 in your cluster. \\r\\nTo test Aurora Serverless v2, you add a reader to your Aurora DB cluster and select Serverless \\r\\nv2 as the instance type. Once the reader is created and available, you can start using it for read\\x02only workloads. Once you confirm that the reader is working as expected, you can initiate a \\r\\nfailover to start using Aurora Serverless v2 for both reads and writes. This option provides a \\r\\nminimal downtime experience to get started with Aurora Serverless v2.\\r\\nCan I migrate from Aurora Serverless v1 to Aurora Serverless v2?\\r\\nYes, you can migrate from Aurora Serverless v1 to Aurora Serverless v2. Refer to the Aurora \\r\\nUser Guide to learn more.\\r\\nWhich versions of Amazon Aurora are supported for Aurora Serverless?\\r\\nAurora Serverless v1 compatibility information can be seen here. Aurora Serverless v2 \\r\\ncompatibility information can be seen here.\\r\\nCan I migrate an existing Aurora DB cluster to Aurora Serverless?\\r\\nYes, you can restore a snapshot taken from an existing Aurora provisioned cluster into an Aurora \\r\\nServerless DB Cluster and the other way around.\\r\\nHow do I connect to an Aurora Serverless DB cluster?\\n\", metadata={'source': './Data\\\\Amazon Aurora FAQs.pdf', 'page': 15}),\n",
       "  Document(page_content='Performance\\r\\nWhat does \"five times the performance of MySQL\" mean?\\r\\nAmazon Aurora delivers significant increases over MySQL performance by tightly integrating \\r\\nthe database engine with an SSD-based virtualized storage layer purpose-built for database \\r\\nworkloads, reducing writes to the storage system, minimizing lock contention, and eliminating \\r\\ndelays created by database process threads.\\r\\nOur tests with SysBench on r3.8xlarge instances show that Amazon Aurora delivers over \\r\\n500,000 SELECTs/sec and 100,000 UPDATEs/sec, five times higher than MySQL running the \\r\\nsame benchmark on the same hardware. Detailed instructions on this benchmark and how to \\r\\nreplicate it yourself are provided in the Amazon Aurora MySQL-Compatible Edition \\r\\nPerformance Benchmarking Guide.\\r\\nWhat does \"three times the performance of PostgreSQL\" mean?\\r\\nAmazon Aurora delivers significant increases over PostgreSQL performance by tightly \\r\\nintegrating the database engine with an SSD-based virtualized storage layer purpose-built for \\r\\ndatabase workloads, reducing writes to the storage system, minimizing lock contention, and \\r\\neliminating delays created by database process threads.\\r\\nOur tests with SysBench on r4.16xlarge instances show that Amazon Aurora delivers \\r\\nSELECTs/sec and UPDATEs/sec over three times higher than PostgreSQL running the same \\r\\nbenchmark on the same hardware. Detailed instructions on this benchmark and how to replicate \\r\\nit yourself are provided in the Amazon Aurora PostgreSQL-Compatible Edition Performance \\r\\nBenchmarking Guide.\\r\\nHow do I optimize my database workload for Amazon Aurora MySQL\\x02Compatible Edition?\\r\\nAmazon Aurora is designed to be compatible with MySQL so that existing MySQL applications \\r\\nand tools can run without requiring modification. However, one area where Amazon Aurora \\r\\nimproves upon MySQL is with highly concurrent workloads. In order to maximize your \\r\\nworkload’s throughput on Amazon Aurora, we recommend building your applications to drive a \\r\\nlarge number of concurrent queries and transactions.\\r\\nHow do I optimize my database workload for Amazon Aurora PostgreSQL\\x02Compatible Edition?\\r\\nAmazon Aurora is designed to be compatible with PostgreSQL so that existing PostgreSQL \\r\\napplications and tools can run without requiring modification. However, one area where Amazon \\r\\nAurora improves upon PostgreSQL is with highly concurrent workloads. In order to maximize \\r\\nyour workload’s throughput on Amazon Aurora, we recommend building your applications to \\r\\ndrive a large number of concurrent queries and transactions.\\n', metadata={'source': './Data\\\\Amazon Aurora FAQs.pdf', 'page': 2})]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided:\n",
      "\n",
      "Aurora offers two configuration options for optimizing your database spend:\n",
      "\n",
      "Aurora Standard:\n",
      "- A cost-effective configuration for applications with low to moderate I/O usage\n",
      "- You pay for database instances, storage, and pay-per-request I/O operations\n",
      "\n",
      "Aurora I/O-Optimized:\n",
      "- Delivers improved price performance for I/O-intensive applications\n",
      "- Ideal for workloads with high I/O variability or where I/O spend exceeds 25% of total Aurora costs\n",
      "- Offers predictable pricing with no charges for read and write I/O operations\n",
      "\n",
      "Key differences:\n",
      "- Aurora Standard charges for I/O operations, while Aurora I/O-Optimized does not\n",
      "- Aurora I/O-Optimized is better suited for I/O-intensive workloads to provide cost savings\n",
      "\n",
      "In summary:\n",
      "- Aurora Standard is a cost-effective option for applications with low to moderate I/O needs\n",
      "- Aurora I/O-Optimized is designed for I/O-intensive workloads to provide improved price performance and predictable costs\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Aurora Standard and Aurora I/O-Optimized?\"\n",
    "result = qa.invoke({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.combine_documents_chain.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'And what is its  utilities like pg_dump/pg_restore ?',\n",
       " 'result': 'Based on the context provided:\\n\\n- The context mentions that to migrate from PostgreSQL to Aurora PostgreSQL-Compatible Edition, you can use the standard pg_dump utility to export data from PostgreSQL and the pg_restore utility to import data into Aurora PostgreSQL.\\n\\n- Specifically, the context states:\\n\\n\"If you want to migrate from PostgreSQL to Aurora and the other way around, you have several options:\\n• You can use the standard pg_dump utility to export data from PostgreSQL and pg_restore utility to import data to Aurora, and the other way around.\"\\n\\nIn summary:\\n\\n- pg_dump is the standard PostgreSQL utility used to export data from a PostgreSQL database.\\n- pg_restore is the standard PostgreSQL utility used to import data into a PostgreSQL database, including an Aurora PostgreSQL-Compatible Edition database.\\n- These utilities allow you to migrate data between a PostgreSQL database and an Aurora PostgreSQL-Compatible Edition database.',\n",
       " 'source_documents': [Document(page_content='You can learn more about the TLE for PostgreSQL project on the official TLE GitHub page.\\r\\nGenera;ve AI\\r\\nWhat is pgvector?\\r\\npgvector is an open-source extension for PostgreSQL supported by Amazon Aurora \\r\\nPostgreSQL-Compatible Edition.\\r\\nWhat capabilities does pgvector enable for Aurora PostgreSQL?\\r\\nYou can use pgvector to store, search, index, and query billions of embeddings that are \\r\\ngenerated from machine learning (ML) and ar3ficial intelligence (AI) models in your database, \\r\\nsuch as those from Amazon Bedrock (limited preview) or Amazon SageMaker. A vector \\r\\nembedding is a numerical representa3on that represents the seman3c meaning of content such \\r\\nas text, images, and video. \\r\\nWith pgvector, you can query embeddings in your Aurora PostgreSQL database to perform \\r\\nefficient seman3c similarity searches of these data types, represented as vectors, combined \\r\\nwith other tabular data in Aurora. This enables the use of genera3ve AI and other AI/ML \\r\\nsystems for new types of applica3ons such as personalized recommenda3ons based on similar \\r\\ntext descrip3ons or images, candidate match based on interview notes, customer service next \\r\\nbest ac3on recommenda3ons based on successful transcripts or chat session dialogs, and \\r\\nmore.\\r\\nRead our blog on vector database capabili3es and learn how to store embeddings using the \\r\\npgvector extension in an Aurora PostgreSQL database, create an interac3ve ques3on answering \\r\\nchatbot, and use the na3ve integra3on between pgvector and Aurora machine learning for \\r\\nsen3ment analysis. \\r\\nDoes pgvector work with Aurora machine learning?\\r\\nYes. Aurora machine learning (ML) exposes ML models as SQL functions, allowing you to use \\r\\nstandard SQL to call ML models, pass data to them, and return predictions as query results. \\r\\npgvector requires vector embeddings to be stored in the database, which requires running the ML \\r\\nmodel on source text or image data to generate embeddings and then moving the embeddings in \\r\\nbatch into Aurora PostgreSQL. \\r\\nAurora ML can make this a real-time process enabling embeddings to be kept up-to-date in \\r\\nAurora PostgreSQL by making periodic calls to Amazon SageMaker which returns the most \\r\\nrecent embeddings from your model.\\n', metadata={'source': './Data\\\\Amazon Aurora FAQs.pdf', 'page': 24}),\n",
       "  Document(page_content='No, Amazon RDS Blue/Green Deployments do not support Amazon Aurora Global Databases.\\r\\nCan I use Amazon RDS Blue/Green Deployments to rollback changes?\\r\\nNo, at this time you cannot use Amazon RDS Blue/Green Deployments to rollback changes.\\r\\nTrusted Language Extensions for PostgreSQL\\r\\nWhy should I use Trusted Language Extensions for PostgreSQL?\\r\\nTrusted Language Extensions (TLE) for PostgreSQL enables developers to build high \\r\\nperformance PostgreSQL extensions and run them safely on Amazon Aurora. In doing so, TLE \\r\\nimproves your time to market and removes the burden placed on database administrators to \\r\\ncertify custom and third-party code for use in production database workloads. You can move \\r\\nforward as soon as you decide an extension meets your needs. With TLE, independent software \\r\\nvendors (ISVs) can provide new PostgreSQL extensions to customers running on Aurora.\\r\\nWhat are traditional risks of running extensions in PostgreSQL and how does \\r\\nTLE for PostgreSQL mitigate those risks?\\r\\nPostgreSQL extensions are executed in the same process space for high performance. However, \\r\\nextensions might have sodware defects that can crash the database. \\r\\nTLE for PostgreSQL offers mul3ple layers of protec3on to mi3gate this risk. TLE is designed to \\r\\nlimit access to system resources. The rds_superuser role can determine who is permibed to \\r\\ninstall specific extensions. However, these changes can only be made through the TLE API. TLE is \\r\\ndesigned to limit the impact of an extension defect to a single database connec3on. In addi3on \\r\\nto these safeguards, TLE is designed to provide DBAs in the rds_superuser role fine-grained, \\r\\nonline control over who can install extensions and they can create a permissions model for \\r\\nrunning them. Only users with sufficient privileges will be able to run and create using the \\r\\n“CREATE EXTENSION” command on a TLE extension. DBAs can also allow-list “PostgreSQL \\r\\nhooks” required for more sophis3cated extensions that modify the database’s internal behavior \\r\\nand typically require elevated privilege. \\r\\nHow does TLE for PostgreSQL relate to/work with other AWS services?\\r\\nTLE for PostgreSQL is available for Amazon Aurora PostgreSQL-Compatible Edition on \\r\\nversions 14.5 and higher. TLE is implemented as a PostgreSQL extension itself and you can \\r\\nactivate it from the rds_superuser role similar to other extensions supported on Aurora.\\r\\nIn what versions of PostgreSQL can I run TLE for PostgreSQL? \\r\\nYou can run TLE for PostgreSQL in PostgreSQL 14.5 or higher in Amazon Aurora.\\n', metadata={'source': './Data\\\\Amazon Aurora FAQs.pdf', 'page': 22}),\n",
       "  Document(page_content='In what Regions is Trusted Language Extensions for PostgreSQL available? \\r\\nTLE for PostgreSQL is currently available in all AWS Regions (excluding AWS China Regions) \\r\\nand the AWS GovCloud Regions.\\r\\nHow much does it cost to run TLE? \\r\\nTLE for PostgreSQL is available to Aurora customers at no additional cost.\\r\\nHow is TLE for PostgreSQL different from extensions available on Amazon \\r\\nAurora and Amazon RDS today?\\r\\nAurora and Amazon RDS support a curated set of over 85 PostgreSQL extensions. AWS \\r\\nmanages the security risks for each of these extensions under the AWS shared responsibility \\r\\nmodel. The extension that implements TLE for PostgreSQL is included in this set. Extensions \\r\\nthat you write or that you obtain from third-party sources and install in TLE are considered part \\r\\nof your application code. You are responsible for the security of your applications that use TLE \\r\\nextensions.\\r\\nWhat are some examples of extensions I could run with TLE for PostgreSQL?\\r\\nYou can build developer functions, such as bitmap compression and differential privacy (such as \\r\\npublicly accessible statistical queries that protect privacy of individuals).\\r\\nWhat programming languages can I use to develop TLE for PostgreSQL?\\r\\nTLE for PostgreSQL currently supports JavaScript, PL/pgSQL, Perl, and SQL.\\r\\nHow do I deploy a TLE for PostgreSQL extension?\\r\\nOnce the rds_superuser role activates TLE for PostgreSQL, you can deploy TLE extensions \\r\\nusing the SQL CREATE EXTENSION command from any PostgreSQL client, such as psql. \\r\\nThis is similar to how you would create a user-defined function written in a procedural language, \\r\\nsuch as PL/pgSQL or PL/Perl. You can control which users have permission to deploy TLE \\r\\nextensions and use specific extensions.\\r\\nHow do TLE for PostgreSQL extensions communicate with the PostgreSQL \\r\\ndatabase?\\r\\nTLE for PostgreSQL access your PostgreSQL database exclusively through the TLE API. The \\r\\nTLE supported trusted languages include all functions of the PostgreSQL server programming \\r\\ninterface (SPI) and support for PostgreSQL hooks, including the check password hook.\\r\\nWhere can I learn more about the TLE for PostgreSQL open-source project?\\n', metadata={'source': './Data\\\\Amazon Aurora FAQs.pdf', 'page': 23}),\n",
       "  Document(page_content='How do I get started with Aurora? \\r\\nTo try Aurora, sign in to the AWS Management Console, select RDS under the Database \\r\\ncategory, and choose Amazon Aurora as your database engine. For detailed guidance and \\r\\nresources, check out our Getting started with Aurora page.\\r\\nIn which AWS Regions is Aurora available?\\r\\nYou can see Region availability for Aurora here.\\r\\nHow can I migrate from MySQL to Aurora and the other way around?\\r\\nIf you want to migrate from MySQL to Aurora and the other way around, you have several \\r\\noptions:\\r\\n• You can use the standard mysqldump u3lity to export data from MySQL and \\r\\nmysqlimport u3lity to import data to Aurora, and the other way around.\\r\\n• You can also use Amazon RDS DB Snapshot migra3on feature to migrate an Amazon RDS \\r\\nfor MySQL DB Snapshot to Aurora using the AWS Management Console.\\r\\nMigration to Aurora completes for most customers in under an hour, though the duration \\r\\ndepends on format and dataset size. For more information see Best Practices for Migrating \\r\\nMySQL Databases to Amazon Aurora.\\r\\nHow can I migrate from PostgreSQL to Aurora and the other way around?\\r\\nIf you want to migrate from PostgreSQL to Aurora and the other way around, you have several \\r\\noptions:\\r\\n• You can use the standard pg_dump u3lity to export data from PostgreSQL and \\r\\npg_restore u3lity to import data to Aurora, and the other way around.\\r\\n• You can also use RDS DB Snapshot migra3on feature to migrate an Amazon RDS for \\r\\nPostgreSQL DB Snapshot to Aurora using the AWS Management Console.\\r\\nMigration to Aurora completes for most customers in under an hour, though the duration \\r\\ndepends on format and dataset size.\\r\\nTo migrate SQL Server databases to Amazon Aurora PostgreSQL-Compatible Edition, you can \\r\\nuse Babelfish for Aurora PostgreSQL. Your applications will work without any changes. See the \\r\\nBabelfish documentation for more information.\\r\\nDo I need to change client drivers to use Amazon Aurora PostgreSQL\\x02Compatible Edition?\\r\\nNo, Aurora works with standard PostgreSQL database drivers.\\n', metadata={'source': './Data\\\\Amazon Aurora FAQs.pdf', 'page': 1}),\n",
       "  Document(page_content='What is Amazon Aurora?\\r\\nAmazon Aurora is a modern relational database service offering performance and high \\r\\navailability at scale, fully open-source MySQL- and PostgreSQL-compatible editions, and a \\r\\nrange of developer tools for building serverless and machine learning (ML)-driven applications.\\r\\nAurora features a distributed, fault-tolerant, and self-healing storage system that is decoupled \\r\\nfrom compute resources and auto-scales up to 128 TiB per database instance. It delivers high \\r\\nperformance and availability with up to 15 low-latency read replicas, point-in-time recovery, \\r\\ncontinuous backup to Amazon Simple Storage Service (Amazon S3), and replication across three \\r\\nAvailability Zones (AZs).\\r\\nAurora is also a fully managed service that automates time-consuming administration tasks such \\r\\nas hardware provisioning, database setup, patching, and backups while providing the security, \\r\\navailability, and reliability of commercial databases at one-tenth of the cost.\\r\\nIs Amazon Aurora MySQL compatible?\\r\\nAmazon Aurora is drop-in compatible with existing MySQL open-source databases and adds \\r\\nsupport for new releases regularly. This means you can easily migrate MySQL databases to and \\r\\nfrom Aurora using standard import/export tools or snapshots. It also means that most of the code, \\r\\napplications, drivers, and tools you already use with MySQL databases today can be used with \\r\\nAurora with little or no change. This makes it easy to move applications between the two \\r\\nengines.\\r\\nYou can see the current Amazon Aurora MySQL release compatibility information in the \\r\\ndocumentation.\\r\\nIs Amazon Aurora PostgreSQL compatible? \\r\\nAmazon Aurora is drop-in compatible with existing PostgreSQL open-source databases and adds \\r\\nsupport for new releases regularly. This means you can easily migrate PostgreSQL databases to \\r\\nand from Aurora using standard import/export tools or snapshots. It also means that most of the \\r\\ncode, applications, drivers, and tools you already use with PostgreSQL databases today can be \\r\\nused with Aurora with little or no change.\\r\\nYou can see the current Amazon Aurora PostgreSQL release compatibility information in the \\r\\ndocumentation.\\r\\nHow is Aurora PostgreSQL supported for issues related to PostgreSQL \\r\\nextensions?\\r\\nAmazon fully supports Aurora PostgreSQL and all extensions available with Aurora. If you need \\r\\nsupport for Aurora PostgreSQL, reach out to AWS Support. If you have an active AWS \\r\\nPremium Support account, you can contact AWS Premium Support for Aurora specific issues.\\n', metadata={'source': './Data\\\\Amazon Aurora FAQs.pdf', 'page': 0})]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa({\"query\":\"And what is its  utilities like pg_dump/pg_restore ?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_template = \"\"\" Given the following conversation and a follow up question, rephrase the follow up question to be a \n",
    "standalone question without changing the content in given question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question: Assistant:\"\"\"\n",
    "\n",
    "condense_question_prompt_template = PromptTemplate.from_template(_template)\n",
    "qa_prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "question_generator = LLMChain(llm=llm, prompt=condense_question_prompt_template, memory=memory)\n",
    "doc_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=qa_prompt)\n",
    "qa_chain = ConversationalRetrievalChain(\n",
    "    retriever=faiss_index.as_retriever(search_kwargs={'k': 5}),\n",
    "    question_generator=question_generator,\n",
    "    combine_docs_chain=doc_chain,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the context provided:\\n\\nAgents for Amazon Bedrock are:\\n\\n- Fully managed capabilities that make it easier for developers to create generative AI-based applications that can complete complex tasks for a wide range of use cases.\\n\\n- They automatically break down tasks and create an orchestration plan without any manual coding.\\n\\n- The agents securely connect to company data through an API, automatically converting data into a machine-readable format, and augmenting the request with relevant information to generate the most accurate response.\\n\\n- Agents can then automatically call APIs to fulfill a user's request, such as tracking inventory levels, sales data, supply chain information, and recommending optimal reorder points and quantities.\\n\\n- As fully managed capabilities, agents for Amazon Bedrock remove the undifferentiated lifting of managing system integration and infrastructure provisioning, allowing developers to use generative AI to its full extent throughout their organization.\\n\\nIn summary, agents for Amazon Bedrock are fully managed capabilities that simplify the development of generative AI applications by automatically orchestrating complex tasks, securely connecting to company data sources, and executing requests without requiring manual coding or infrastructure management.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = 'what are agents for amazon bedrock?'\n",
    "result = qa_chain({'question': question, 'chat_history': chat_history})\n",
    "response = result['answer']\n",
    "chat_history.append((question, response))\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the context provided:\\n\\nAgents for Amazon Bedrock are defined as:\\n\\n- Fully managed capabilities that make it easier for developers to create generative AI-based applications\\n- They can complete complex tasks for a wide range of use cases and deliver up-to-date answers based on proprietary knowledge sources\\n- With just a few clicks, agents for Amazon Bedrock automatically break down tasks and create an orchestration plan, without any manual coding\\n- The agent securely connects to company data through an API, automatically converting data into a machine-readable format, and augmenting the request with relevant information to generate the most accurate response\\n- Agents can then automatically call APIs to fulfill a user's request\\n- As fully managed capabilities, agents for Amazon Bedrock remove the undifferentiated lifting of managing system integration and infrastructure provisioning, allowing developers to use generative AI to its full extent throughout their organization\\n\\nIn summary, agents for Amazon Bedrock are fully managed capabilities that simplify the development of generative AI applications by automatically orchestrating complex tasks, securely integrating with company data sources, and executing requests without requiring manual coding or infrastructure management.\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = qa_chain({'question': \"And what is the definition of an agents for amazon bedrock?\", 'chat_history': chat_history})\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what are agents for amazon bedrock?',\n",
       "  \"Based on the context provided:\\n\\nAgents for Amazon Bedrock are:\\n\\n- Fully managed capabilities that make it easier for developers to create generative AI-based applications that can complete complex tasks for a wide range of use cases.\\n\\n- They automatically break down tasks and create an orchestration plan without any manual coding.\\n\\n- The agents securely connect to company data through an API, automatically converting data into a machine-readable format, and augmenting the request with relevant information to generate the most accurate response.\\n\\n- Agents can then automatically call APIs to fulfill a user's request, such as tracking inventory levels, sales data, supply chain information, and recommending optimal reorder points and quantities.\\n\\n- As fully managed capabilities, agents for Amazon Bedrock remove the undifferentiated lifting of managing system integration and infrastructure provisioning, allowing developers to use generative AI to its full extent throughout their organization.\\n\\nIn summary, agents for Amazon Bedrock are fully managed capabilities that simplify the development of generative AI applications by automatically orchestrating complex tasks, securely connecting to company data sources, and executing requests without requiring manual coding or infrastructure management.\")]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "lang-pip",
   "language": "python",
   "name": "lang-pip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
